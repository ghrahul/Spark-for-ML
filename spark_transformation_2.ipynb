{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark data cleaning and engineering\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, State: string, Country: string, Lat: double, Long: double, Date: string, Confirmed: int, Death: int, Recovered: int, state_cleaned: string, City: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_location = \"datasets/corona_data.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"nanValue\", ' ') \\\n",
    "  .option(\"nullValue\", ' ') \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------------------+--------+---------+----------+---------+-----+---------+--------------------+----+\n",
      "|_c0|           State|             Country|     Lat|     Long|      Date|Confirmed|Death|Recovered|       state_cleaned|City|\n",
      "+---+----------------+--------------------+--------+---------+----------+---------+-----+---------+--------------------+----+\n",
      "|  0|            null|            Thailand|    15.0|    101.0|2020-01-22|        2|    0|        0|             Bangkok|null|\n",
      "|  1|            null|               Japan|    36.0|    138.0|2020-01-22|        2|    0|        0|             Hiraide|null|\n",
      "|  2|            null|           Singapore|  1.2833| 103.8333|2020-01-22|        0|    0|        0|           Singapore|null|\n",
      "|  3|            null|               Nepal| 28.1667|    84.25|2020-01-22|        0|    0|        0|           Kathmandu|null|\n",
      "|  4|            null|            Malaysia|     2.5|    112.5|2020-01-22|        0|    0|        0|             Sarawak|null|\n",
      "|  5|British Columbia|              Canada| 49.2827|-123.1207|2020-01-22|        0|    0|        0|    British Columbia|null|\n",
      "|  6| New South Wales|           Australia|-33.8688| 151.2093|2020-01-22|        0|    0|        0|     New South Wales|null|\n",
      "|  7|        Victoria|           Australia|-37.8136| 144.9631|2020-01-22|        0|    0|        0|            Victoria|null|\n",
      "|  8|      Queensland|           Australia|-28.0167|    153.4|2020-01-22|        0|    0|        0|          Queensland|null|\n",
      "|  9|            null|            Cambodia|   11.55| 104.9167|2020-01-22|        0|    0|        0|          Phnom Penh|null|\n",
      "| 10|            null|           Sri Lanka|     7.0|     81.0|2020-01-22|        0|    0|        0|Sri Jayawardenapu...|null|\n",
      "| 11|            null|             Germany|    51.0|      9.0|2020-01-22|        0|    0|        0|              Berlin|null|\n",
      "| 12|            null|             Finland|    64.0|     26.0|2020-01-22|        0|    0|        0|            Helsinki|null|\n",
      "| 13|            null|United Arab Emirates|    24.0|     54.0|2020-01-22|        0|    0|        0|           Abu Dhabi|null|\n",
      "| 14|            null|         Philippines|    13.0|    122.0|2020-01-22|        0|    0|        0|              Manila|null|\n",
      "| 15|            null|               India|    21.0|     78.0|2020-01-22|        0|    0|        0|           New Delhi|null|\n",
      "| 16|            null|               Italy|    43.0|     12.0|2020-01-22|        0|    0|        0|                Rome|null|\n",
      "| 17|            null|              Sweden|    63.0|     16.0|2020-01-22|        0|    0|        0|           Stockholm|null|\n",
      "| 18|            null|               Spain|    40.0|     -4.0|2020-01-22|        0|    0|        0|              Toledo|null|\n",
      "| 19| South Australia|           Australia|-34.9285| 138.6007|2020-01-22|        0|    0|        0|     South Australia|null|\n",
      "+---+----------------+--------------------+--------+---------+----------+---------+-----+---------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28143"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_df_max = df.join(df.groupBy(\"Country\", \"State_cleaned\").agg(F.max(\"Date\").alias(\"Date\")), on=[\"Country\", \"State_cleaned\",\n",
    "                                                                                                    \"Date\"], how=\"inner\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+-----+----------------+--------+---------+---------+-----+---------+----+\n",
      "|             Country|       state_cleaned|      Date|  _c0|           State|     Lat|     Long|Confirmed|Death|Recovered|City|\n",
      "+--------------------+--------------------+----------+-----+----------------+--------+---------+---------+-----+---------+----+\n",
      "|            Thailand|             Bangkok|2020-03-20|27666|            null|    15.0|    101.0|      322|    1|       42|null|\n",
      "|               Japan|             Hiraide|2020-03-20|27667|            null|    36.0|    138.0|      963|   33|      191|null|\n",
      "|           Singapore|           Singapore|2020-03-20|27668|            null|  1.2833| 103.8333|      385|    0|      124|null|\n",
      "|               Nepal|           Kathmandu|2020-03-20|27669|            null| 28.1667|    84.25|        1|    0|        1|null|\n",
      "|            Malaysia|             Sarawak|2020-03-20|27670|            null|     2.5|    112.5|     1030|    3|       87|null|\n",
      "|              Canada|    British Columbia|2020-03-20|27671|British Columbia| 49.2827|-123.1207|      271|    8|        4|null|\n",
      "|           Australia|     New South Wales|2020-03-20|27672| New South Wales|-33.8688| 151.2093|      353|    6|        4|null|\n",
      "|           Australia|            Victoria|2020-03-20|27673|        Victoria|-37.8136| 144.9631|      121|    0|        8|null|\n",
      "|           Australia|          Queensland|2020-03-20|27674|      Queensland|-28.0167|    153.4|      184|    0|        8|null|\n",
      "|            Cambodia|          Phnom Penh|2020-03-20|27675|            null|   11.55| 104.9167|       51|    0|        1|null|\n",
      "|           Sri Lanka|Sri Jayawardenapu...|2020-03-20|27676|            null|     7.0|     81.0|       73|    0|        3|null|\n",
      "|             Germany|              Berlin|2020-03-20|27677|            null|    51.0|      9.0|    19848|   67|      180|null|\n",
      "|             Finland|            Helsinki|2020-03-20|27678|            null|    64.0|     26.0|      450|    0|       10|null|\n",
      "|United Arab Emirates|           Abu Dhabi|2020-03-20|27679|            null|    24.0|     54.0|      140|    2|       31|null|\n",
      "|         Philippines|              Manila|2020-03-20|27680|            null|    13.0|    122.0|      230|   18|        8|null|\n",
      "|               India|           New Delhi|2020-03-20|27681|            null|    21.0|     78.0|      244|    5|       20|null|\n",
      "|               Italy|                Rome|2020-03-20|27682|            null|    43.0|     12.0|    47021| 4032|     4440|null|\n",
      "|              Sweden|           Stockholm|2020-03-20|27683|            null|    63.0|     16.0|     1639|   16|       16|null|\n",
      "|               Spain|              Toledo|2020-03-20|27684|            null|    40.0|     -4.0|    20410| 1043|     1588|null|\n",
      "|           Australia|     South Australia|2020-03-20|27685| South Australia|-34.9285| 138.6007|       50|    0|        3|null|\n",
      "+--------------------+--------------------+----------+-----+----------------+--------+---------+---------+-----+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corona_df_max.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+\n",
      "|  Country|sum(Confirmed)|sum(Recovered)|\n",
      "+---------+--------------+--------------+\n",
      "|    China|       3287028|       1570179|\n",
      "|Australia|          4996|           601|\n",
      "+---------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Country\", \"State_cleaned\", \"Confirmed\", \"Recovered\").filter(col(\"Country\").isin(\"Australia\", \"China\")).groupBy(\"Country\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+\n",
      "|  Country|sum(Confirmed)|sum(Recovered)|\n",
      "+---------+--------------+--------------+\n",
      "|Australia|          4996|           601|\n",
      "|     null|       3292024|       1570780|\n",
      "|    China|       3287028|       1570179|\n",
      "+---------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Country\", \"State_cleaned\", \"Confirmed\", \"Recovered\").filter(col(\"Country\").isin(\"Australia\", \"China\")).cube(\"Country\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------+--------------+\n",
      "|  Country|     State_cleaned|sum(Confirmed)|sum(Recovered)|\n",
      "+---------+------------------+--------------+--------------+\n",
      "|    China|          Shandong|         31306|         18332|\n",
      "|    China|         Chongqing|         26929|         16257|\n",
      "|    China|           Ningxia|          3257|          2351|\n",
      "|    China|              null|       3287028|       1570179|\n",
      "|Australia|Northern Territory|            15|             0|\n",
      "|    China|          Shanghai|         16214|         10137|\n",
      "|    China|             Anhui|         44677|         28737|\n",
      "|    China|           Qinghai|           893|           662|\n",
      "|    China|         Hong Kong|          4314|          1485|\n",
      "|    China|            Shanxi|          6199|          4055|\n",
      "|    China|            Hainan|          7740|          4864|\n",
      "|     null|              null|       3292024|       1570780|\n",
      "|    China|           Guangxi|         11590|          6437|\n",
      "|    China|           Jiangxi|         42374|         27215|\n",
      "|Australia|          Tasmania|            78|             6|\n",
      "|    China|           Jiangsu|         28375|         18756|\n",
      "|    China|            Yunnan|          8286|          5110|\n",
      "|Australia|   New South Wales|          2179|           176|\n",
      "|    China|             Hubei|       2691485|       1201137|\n",
      "|Australia|        Queensland|          1019|           141|\n",
      "+---------+------------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Country\", \"State_cleaned\", \"Confirmed\", \"Recovered\").filter(col(\"Country\").isin(\"Australia\", \"China\")).rollup(\"Country\", \"State_cleaned\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, State: string, Country: string, Lat: double, Long: double, Date: string, Confirmed: int, Death: int, Recovered: int, state_cleaned: string, City: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 ms ± 2.81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 14 20:27:09 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 451.22       Driver Version: 451.22       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1650   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P8     6W /  N/A |    134MiB /  4096MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rapid_cudf.jar', <http.client.HTTPMessage at 0x1f677fcf9c8>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = 'https://repo1.maven.org/maven2/ai/rapids/cudf/0.9.2/cudf-0.9.2-cuda10-1.jar'\n",
    "filename = 'rapid_cudf.jar'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rapid_xgboost.jar', <http.client.HTTPMessage at 0x1f677f8b208>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://repo1.maven.org/maven2/ai/rapids/xgboost4j_2.x/1.0.0-Beta5/xgboost4j_2.x-1.0.0-Beta5.jar'\n",
    "filename = 'rapid_xgboost.jar'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rapid_xgboost_spark.jar', <http.client.HTTPMessage at 0x1f677f6bbc8>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://repo1.maven.org/maven2/ai/rapids/xgboost4j-spark_2.x/1.0.0-Beta5/xgboost4j-spark_2.x-1.0.0-Beta5.jar'\n",
    "filename = 'rapid_xgboost_spark.jar'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://repo1.maven.org/maven2/ai/rapids/cudf/0.9.2/cudf-0.9.2-cuda10-1.jar\n",
    "# !wget https://repo1.maven.org/maven2/ai/rapids/xgboost4j_2.x/1.0.0-Beta5/xgboost4j_2.x-1.0.0-Beta5.jar\n",
    "# !wget https://repo1.maven.org/maven2/ai/rapids/xgboost4j-spark_2.x/1.0.0-Beta5/xgboost4j-spark_2.x-1.0.0-Beta5.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars rapid_cudf.jar,rapid_xgboost.jar,rapid_xgboost_spark.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
